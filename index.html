
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Copy Text Buttons</title>
    <style>
        body {
            background-color: #f0f0f0;
            font-family: Arial, sans-serif;
            text-align: center;
        }

        button {
            margin: 10px;
            padding: 10px;
            font-size: 16px;
            background-color: #ddd;
            border: 1px solid #aaa;
            cursor: pointer;
        }

        button:hover {
            background-color: #ccc;
        }

        #copiedMsg {
            margin-top: 20px;
            font-weight: bold;
            color: green;
        }
    </style>
</head>

<body>

   

    <!-- Buttons for each code block -->
    <button onclick="copyText(text8)">fixer</button>
    <button onclick="copyText(text9)">binbuck</button>
    <button onclick="copyText(text10)">aggreg</button>
    <button onclick="copyText(text11)">outlier</button>
    <button onclick="copyText(text12)">audit</button>
    <button onclick="copyText(text13)">dataprocessR</button>
    <button onclick="copyText(text14)">retrieveAttributes</button>
    <button onclick="copyText(text15)">DataPattern</button>
    <button onclick="copyText(text16)">LoadingIP_DATA</button>
    <button onclick="copyText(text17)">errorManage</button>
    

    <p id="copiedMsg"></p>

    <script>
        // Text blocks (only a few shown here for demonstration; add all your blocks similarly)

		 const text8 = `
        # Program to Demonstrate Fixers utilities
import string
import datetime as dt

# 1. Removing leading and lagging spaces from data entry
baddata = "  Data Science with too many spaces is bad!!!"
print('>', baddata, '<')  # Original data with spaces
cleandata = baddata.strip()
print('>', cleandata, '<')  # Cleaned data without spaces

# 2. Removing nonprintable characters from data entry
printable = set(string.printable)
baddata = "Data\x00science with\x02 funny characters is \x10bad!!!"
cleandata = "".join(filter(lambda x: x in printable, baddata))
print('Bad Data: ', baddata)
print('Clean Data: ', cleandata)

# 3. Reformatting data entry to match specific formatting criteria (convert YYYY-MM-DD to DD Month YYYY)
baddate = dt.date(2019, 10, 31)
baddata = format(baddate, '%Y-%m-%d')  # Original date format
gooddate = dt.datetime.strptime(baddata, '%Y-%m-%d')  # Parse to datetime object
gooddata = format(gooddate, '%d-%B-%Y')  # Reformat to desired format
print('Bad Data: ', baddata)
print('Clean Data: ', gooddata

        `
const text9 = `
        # Data Binning and Bucketing
import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import scipy.stats as stats
np.random.seed(0)
#example data
mu=90#mean of distribution
sigma=25#sd of deviation
x=mu+sigma*np.random.randn(5000)
num_bins=25
fig,ax=plt.subplots()

#the histogram of the data
n, bins, patches = ax.hist(x, num_bins, density=1)

#add a best-fit line
y=stats.norm.pdf(bins,mu,sigma)
ax.plot(bins,y,'--')
ax.set_xlabel('Example Data')
ax.set_ylabel('Probability Density')
sTitle = r'Histogram '+str(len(x)) + 'entries intro' + str(num_bins) + 'Bins:$\mu=' +str(mu) +'$,$\sigma=' + str(sigma) + '$'
ax.set_title(sTitle)
fig.tight_layout()
sPathFig='C:/Users/NANDINI/OneDrive/Documents/Roshni/Java/Histogram.png'
fig.savefig(sPathFig)
plt.show()

        `
 const text10 = `
        # Averaging of Data
import pandas as pd

# Input and Output file paths
InputFileName = 'C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects/Python_Diwali_Sales_Analysis/Diwali Sales Data.csv'
OutputFileName = 'C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects/Python_Diwali_Sales_Analysis/Diwali_Sales_Data_Avg.csv'

Base = 'C:/Users/NANDINI'
print('#################')
print('Working Base: ', Base, 'using')
print('#################')

# Use the InputFileName directly since it already contains the full path
sFileName = InputFileName
print('Loading: ', sFileName)

# Read the CSV file with specified columns
IP_DATA_ALL = pd.read_csv(
    sFileName,
    header=0,
    low_memory=False,
    usecols=['User_ID', 'Cust_name', 'Product_ID', 'Gender'],
    encoding="latin-1"
)

# Rename columns for consistency
IP_DATA_ALL.rename(columns={'Customer Name': 'Cust_name'}, inplace=True)

# Select specific columns
AllData = IP_DATA_ALL[['User_ID', 'Cust_name', 'Gender']]

# Print all data
print(AllData)

# Group by 'Country' and 'Place_Name' and calculate the mean of 'Latitude'
MeanData = AllData.groupby(['User_ID', 'Cust_name'])['Product_ID'].mean()

# Print the grouped mean data
print(MeanData)

# Optionally save the output to a new file
MeanData.to_csv(OutputFileName, header=True)
print('Processed data saved to:', OutputFileName)
        `

                const text11 = `
        import pandas as pd

# Input and Output filenames
InputFileName = "Diwali Sales Data.csv"
OutputFileName = "Retrieve_Outliers.csv"
Base = "C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects/Python_Diwali_Sales_Analysis"

print("############################################")
print("Working Base :", Base)
print("############################################")

# Load the input file with the correct path
file_path = f"{Base}/{InputFileName}"
print("Loading :", file_path)
data = pd.read_csv(file_path, encoding='latin-1')

# Assuming we are focusing on a specific column for outliers (e.g., 'Amount')
column_to_analyze = 'Amount'

# Grouping data (if required)
MeanData = data[column_to_analyze].mean()
StdData = data[column_to_analyze].std()

print("Outliers")
UpperBound = MeanData + StdData
LowerBound = MeanData - StdData

print("Higher than ", UpperBound)
OutliersHigher = data[data[column_to_analyze] > UpperBound]
print(OutliersHigher)

print("Lower than ", LowerBound)
OutliersLower = data[data[column_to_analyze] < LowerBound]
print(OutliersLower)

# Combine and export outliers
Outliers = pd.concat([OutliersHigher, OutliersLower])
Outliers.to_csv(f"{Base}/{OutputFileName}", index=False)
        `
        const text12 = `
        import sys
import os
import logging
import uuid
import shutil
import time

###############################################################
Base="C:/Users/NANDINI/OneDrive/Documents/Roshni/Projects"

###############################################################
sCompanies=['01-Vermeulen','02-Krennwallner','03-Hillman','04-Clark']
sLayers=['01-Retrieve','02-Assess','03-Process','04-Transform','05-Organise','06-Report']
sLevels=['debug','info','warning','error']

for sCompany in sCompanies:
    sFileDir=Base + '/' + sCompany
    if not os.path.exists(sFileDir):
        os.makedirs(sFileDir)
    for sLayer in sLayers:
        log = logging.getLogger() # root logger
        for hdlr in log.handlers[:]: # remove all old handlers
            log.removeHandler(hdlr)
        #######################################################
        sFileDir=Base + '/' + sCompany + '/' + sLayer + '/Logging'
        if os.path.exists(sFileDir):
            shutil.rmtree(sFileDir)
        time.sleep(2)
        os.makedirs(sFileDir)

        skey=str(uuid.uuid4())
        sLogFile=Base + '/' + sCompany + '/' + sLayer + '/Logging/Logging_'+skey+'.log'
        print('Set up:', sLogFile)
        
        # set up logging to file
        logging.basicConfig(level=logging.DEBUG,
                            format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',
                            datefmt='%m-%d %H:%M',
                            filename=sLogFile,
                            filemode='w')

        # define a Handler which writes INFO messages or higher to the sys.stderr
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        # set a format which is simpler for console use
        formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
        # tell the handler to use this format
        console.setFormatter(formatter)
        # add the handler to the root logger
        logging.getLogger().addHandler(console)
        
        # Now, we can log to the root logger, or any other logger
        logging.info('Practical Data Science is fun!.')
        
        for sLevel in sLevels:
            sApp='Application-'+ sCompany + '-' + sLayer + '-' + sLevel
            logger = logging.getLogger(sApp)
            if sLevel == 'debug':
                logger.debug('Practical Data Science logged a debugging message.')
            if sLevel == 'info':
                logger.info('Practical Data Science logged information message.')
            if sLevel == 'warning':
                logger.warning('Practical Data Science logged a warning message.')
            if sLevel == 'error':
                logger.error('Practical Data Science logged an error message.')

        `
        const text13 = `
        (R)
library(readr) 
IP_DATA_ALL<- read_csv("C:/Users/Dell/OneDrive/Desktop/DSPractical/IP_DATA_ALL.csv") View(IP_DATA_ALL) 
spec(IP_DATA_ALL) 
library(tibble) 
set_tidy_names(IP_DATA_ALL,syntactic = TRUE,quiet = FALSE) IP_DATA_ALL_FIX=set_tidy_names(IP_DATA_ALL,syntacti c=TRUE,quiet=FALSE) sapply(IP_DATA_ALL_FIX,typeof) 
library(data.table) hist_country=data.table(Country=unique(IP_DATA_ALL_FIX[i s.na(IP_DATA_ALL_FIX['Country'])==0,]$Country))
setorder(hist_country,'Country') hist_country_with_id=rowid_to_column(hist_country,var="Row IDCountry") 
IP_DATA_COUNTRY_FREQ=data.table(with(IP_DATA_AL L_FIX,table(Country))) 
View(IP_DATA_COUNTRY_FREQ) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],min,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Country'],min,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],max,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Country'],max,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],mean,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],median,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],quantile,na.rm=TRUE 
sapply(IP_DATA_ALL_FIX[,'Latitude'],range,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Latitude'],sd,na.rm=TRUE) 
sapply(IP_DATA_ALL_FIX[,'Longitude'],sd,na.rm=TRUE)
        `
        const text14 = `
        import sys 
import os import pandas as pd 
sFileName="C:/Users/Dell/OneDrive/Desktop/DS Practical/IP_DATA_ALL.csv" IP_DATA_ALL=pd.read_csv(sFileName,header=0,low_memor y=False,encoding="latin-1") 
sFileDir="C:/Users/Dell/OneDrive/Desktop/DS Practical" if not os.path.exists(sFileDir):
os.makedirs(sFileDir) 
print('Rows:',IP_DATA_ALL.shape[0]) 
print('Columns:',IP_DATA_ALL.shape[1]) 
print('Raw Data') 
for i in range(0,len(IP_DATA_ALL.columns)):
print(IP_DATA_ALL.columns[i],
type(IP_DATA_ALL.columns [i])) 
print('Fixed Data') IP_DATA_ALL_FIX=IP_DATA_ALL 
for i in range(0,len(IP_DATA_ALL.columns)):
cNameOld=IP_DATA_ALL_FIX.columns[i]+' ' cNameNew=cNameOld.strip().replace(" ",".") IP_DATA_ALL_FIX.columns.values[i]=cNameNew
print(IP_DATA_ALL.columns[i],type(IP_DATA_ALL.columns [i]))
print(IP_DATA_ALL_FIX.head()) print('Fixed Data Set with ID') 
IP_DATA_ALL_with_ID=IP_DATA_ALL_FIX 
IP_DATA_ALL_with_ID.index.names=['RowID'] 
sFileName2='C:/Users/Dell/OneDrive/Desktop/DS Practical/Retrieve_IP_DATA.csv' 
IP_DATA_ALL_with_ID.to_csv(sFileName2,index=True,enco ding="latin-1")
        `
        const text15 = `
        (R)
library(readr) 
library(data.table) 
FileName <- "C:/Users/Dell/OneDrive/Desktop/DS Practical/IP_DATA_ALL_1.csv" IP_DATA_ALL <- read_csv(FileName) 
hist_country <- data.table(Country = unique(IP_DATA_ALL$Country)) 
pattern_country <- data.table(Country = hist_country$Country, PatternCountry = hist_country$Country) 
oldchar <- c(letters, LETTERS, "0", "1", "2", "3", "4", "5", "6", "7", "8", "9", " ", "~", "!", "@", "#", "$", "%", "^", "&", "*", "(", ")", "-", "_", "=", "+", "{", "}", "[", "]", "|", "\\", ":", ";", "'", "\"", "<", ",", ".", ">", "/", "?") 
newchar <- rep("A", length(letters) + length(LETTERS)) # For letters 
newchar <- c(newchar, rep("N", 10)) # For digits
newchar <- c(newchar, "b", rep("u", length(oldchar) - length(letters) - length(LETTERS) - 10 - 1)) # For space and special characters
translation_table <- setNames(newchar, oldchar) 
replace_chars <- function(s, translation_table) { 
for (old in names(translation_table)) { 
s <- chartr(old, translation_table[old], s) 
}
return(s) 
} 
pattern_country[, PatternCountry := sapply(PatternCountry, replace_chars, translation_table)] 
View(pattern_country)

        `
        const text16 = `
        import sys 
import os 
import pandas as pd 
sFileName='C:/Users/Dell/OneDrive/Desktop/DS Practical/IP_DATA_ALL.csv' IP_DATA_ALL=pd.read_csv(sFileName) 
sFileDir='C:/Users/Dell/OneDrive/Desktop/DS Practical' 
if not os.path.exists(sFileDir): 
os.makedirs(sFileDir) 
print('Rows:', IP_DATA_ALL.shape[0]) 
print('Columns:', IP_DATA_ALL.shape[1]) 
print(' Raw Data Set ') 
for i in range(0,len(IP_DATA_ALL.columns)):
print(IP_DATA_ALL.columns[i],
type(IP_DATA_ALL.columns [i])) print(' Fixed Data Set ') IP_DATA_ALL_FIX=IP_DATA_ALL 
for i in range(0,len(IP_DATA_ALL.columns)): 
cNameOld=IP_DATA_ALL_FIX.columns[i] + ' '
cNameNew=cNameOld.strip().replace(' ', '.') 
IP_DATA_ALL_FIX.columns.values[i] = cNameNew print(IP_DATA_ALL.columns[i],
type(IP_DATA_ALL.columns [i])) 
print('Fixed Data Set with ID')
IP_DATA_ALL_with_ID=IP_DATA_ALL_FIX 
IP_DATA_ALL_with_ID.index.names = ['RowID'] print(' Done!! ')

        `
        const text17 = `
        import pandas as pd 
from sklearn.preprocessing 
import StandardScaler 
url = 'https://archive.ics.uci.edu/ml/machine-learningdatabases/iris/iris.data' 
df = pd.read_csv(url, header=None, names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']) 
print("First few rows of the dataset:") 
print(df.head()) 
print("\nDataFrame info:") 
print(df.info()) 
print("\nSummary statistics:") 
print(df.describe()) 
missing_values = df.isnull().sum() 
if missing_values.any():
print("\nMissing values in the dataset:")
print(missing_values[missing_values > 0]) 
else: 
print("\nNo missing values in the dataset.") 
duplicates = df.duplicated().sum() 
print('Total duplicate rows: {duplicates}') 
df_unique = df.drop_duplicates() 
print('Unique rows after removing duplicates: {len(df_unique)}') 
Q1 = df['sepal_length'].quantile(0.25) 
Q3 = df['sepal_length'].quantile(0.75) 
IQR = Q3 - Q1 
outliers = df[(df['sepal_length'] < (Q1 - 1.5 * IQR)) | (df['sepal_length'] > (Q3 + 1.5 * IQR))] print('\nOutliers in sepal_length:\n{outliers}') 
scaler = StandardScaler() 
df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] = scaler.fit_transform(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]) 
df = pd.get_dummies(df, columns=['species'], drop_first=True) 
print("\nFinal check for missing values:") 
print(df.isnull().sum()) 
print('\nFinal count of duplicate rows: {df.duplicated().sum()}')

        `
       
        

        // Add the rest of your text blocks here (text6, text7, ...)

        // Function to copy text using Clipboard API
        async function copyText(text) {
            try {
                // Use Clipboard API to write text to the clipboard
                await navigator.clipboard.writeText(text);

                // Notify user of success
                const messageElement = document.getElementById("copiedMsg");
                messageElement.innerText = "Text copied successfully!";
                messageElement.style.color = "green";

                // Clear message after 2 seconds
                setTimeout(() => {
                    messageElement.innerText = "";
                }, 2000);
            } catch (err) {
                // Notify user of failure
                const messageElement = document.getElementById("copiedMsg");
                messageElement.innerText = "Failed to copy text. Check your browser permissions.";
                messageElement.style.color = "red";

                console.error("Error copying text: ", err);
            }
        }
    </script>

</body>

</html>
